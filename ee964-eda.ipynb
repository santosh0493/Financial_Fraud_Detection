{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6959198,"sourceType":"datasetVersion","datasetId":3997563}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install textstat\n!pip install imbalanced-learn==0.10.1\n!pip install --upgrade scikit-learn\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!!pip install --upgrade imbalanced-learn\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# !/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nFinancial Fraud Text Analysis - Exploratory Data Analysis\nThis script performs exploratory data analysis on financial fraud text data,\nanalyzing text features, sentiment, readability, and addressing class imbalance.\n\"\"\"\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport re\nimport string\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nimport textstat\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTENC\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for plots\nplt.style.use('ggplot')\nsns.set_theme(style=\"whitegrid\")\n\n# Download necessary NLTK resources\n# Fix SSL certificate issues for NLTK downloads\nimport ssl\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\n# Create NLTK data directory if it doesn't exist\nnltk_data_dir = os.path.expanduser('~/nltk_data')\nif not os.path.exists(nltk_data_dir):\n    os.makedirs(nltk_data_dir)\n\n# Download NLTK resources\nprint(\"Downloading NLTK resources...\")\nnltk.download('punkt', quiet=False)\nnltk.download('stopwords', quiet=False)\nnltk.download('punkt_tab', quiet=False)\n\n# Alternative approach for tokenization if punkt_tab is not available\ndef safe_tokenize(text, tokenize_func):\n    \"\"\"Safely tokenize text, falling back to simple split if tokenization fails\"\"\"\n    try:\n        return tokenize_func(text)\n    except LookupError:\n        # Fallback to simple splitting\n        if tokenize_func == word_tokenize:\n            return text.split()\n        elif tokenize_func == sent_tokenize:\n            # Simple sentence splitting by punctuation\n            sentences = re.split(r'(?<=[.!?])\\s+', text)\n            return [s for s in sentences if s.strip()]\n        return []\n\n# Load the dataset\nprint(\"Loading dataset...\")\ndata_path = '/kaggle/input/financial-statement-fraud-data/Final_Dataset.csv'\ndf = pd.read_csv(data_path)\nprint(f\"Dataset shape: {df.shape}\")\n\n# Display basic information\nprint(\"\\nBasic Information:\")\nprint(df.info())\nprint(\"\\nClass distribution:\")\nprint(df['Fraud'].value_counts())\nprint(f\"Class balance ratio: {df['Fraud'].value_counts().min() / df['Fraud'].value_counts().max():.2f}\")\n\n# Create a copy of the dataframe for analysis\ndf_analysis = df.copy()\n\n# Convert target to binary\ndf_analysis['Fraud_Binary'] = df_analysis['Fraud'].map({'yes': 1, 'no': 0})\n\n# Text Preprocessing Function\ndef preprocess_text(text):\n    \"\"\"Basic text preprocessing\"\"\"\n    # Convert to lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(f'[{string.punctuation}]', ' ', text)\n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Apply preprocessing\ndf_analysis['Processed_Text'] = df_analysis['Fillings'].apply(preprocess_text)\n\n# Text Length Analysis\nprint(\"\\nAnalyzing text length distributions...\")\ndf_analysis['Text_Length'] = df_analysis['Fillings'].apply(len)\ndf_analysis['Word_Count'] = df_analysis['Processed_Text'].apply(lambda x: len(safe_tokenize(x, word_tokenize)))\ndf_analysis['Sentence_Count'] = df_analysis['Fillings'].apply(lambda x: len(safe_tokenize(x, sent_tokenize)))\ndf_analysis['Avg_Word_Length'] = df_analysis['Processed_Text'].apply(\n    lambda x: np.mean([len(word) for word in safe_tokenize(x, word_tokenize)]) if safe_tokenize(x, word_tokenize) else 0\n)\ndf_analysis['Avg_Sentence_Length'] = df_analysis['Word_Count'] / df_analysis['Sentence_Count'].replace(0, 1)\n\n# Readability Analysis\nprint(\"Calculating readability metrics...\")\ndf_analysis['Flesch_Reading_Ease'] = df_analysis['Fillings'].apply(textstat.flesch_reading_ease)\ndf_analysis['Flesch_Kincaid_Grade'] = df_analysis['Fillings'].apply(textstat.flesch_kincaid_grade)\ndf_analysis['Automated_Readability_Index'] = df_analysis['Fillings'].apply(textstat.automated_readability_index)\n\n# Sentiment Analysis\nprint(\"Performing sentiment analysis...\")\ndf_analysis['Polarity'] = df_analysis['Fillings'].apply(lambda x: TextBlob(x).sentiment.polarity)\ndf_analysis['Subjectivity'] = df_analysis['Fillings'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n\n# Summary statistics by class\nprint(\"\\nSummary statistics by fraud class:\")\nfraud_stats = df_analysis.groupby('Fraud')[\n    ['Text_Length', 'Word_Count', 'Sentence_Count', 'Avg_Word_Length',\n     'Avg_Sentence_Length', 'Flesch_Reading_Ease', 'Flesch_Kincaid_Grade',\n     'Polarity', 'Subjectivity']\n].agg(['mean', 'median', 'std']).T\nprint(fraud_stats)\n\n# Create a directory for plots if it doesn't exist\nif not os.path.exists('plots'):\n    os.makedirs('plots')\n\n# Visualization functions\ndef plot_distribution(df, column, title, xlabel, ylabel=\"Frequency\", bins=30, figsize=(10, 6)):\n    \"\"\"Plot distribution of a column with fraud/non-fraud distinction\"\"\"\n    plt.figure(figsize=figsize)\n\n    # Plot histograms for each class\n    sns.histplot(data=df[df['Fraud'] == 'yes'], x=column, bins=bins, alpha=0.5, label='Fraud', color='red')\n    sns.histplot(data=df[df['Fraud'] == 'no'], x=column, bins=bins, alpha=0.5, label='Non-Fraud', color='blue')\n\n    plt.title(title, fontsize=15)\n    plt.xlabel(xlabel, fontsize=12)\n    plt.ylabel(ylabel, fontsize=12)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(f'plots/{column}_distribution.png')\n    plt.close()\n\ndef plot_boxplot(df, column, title, ylabel, figsize=(10, 6)):\n    \"\"\"Plot boxplot of a column with fraud/non-fraud distinction\"\"\"\n    plt.figure(figsize=figsize)\n\n    sns.boxplot(x='Fraud', y=column, data=df)\n\n    plt.title(title, fontsize=15)\n    plt.xlabel('Fraud Status', fontsize=12)\n    plt.ylabel(ylabel, fontsize=12)\n    plt.tight_layout()\n    plt.savefig(f'plots/{column}_boxplot.png')\n    plt.close()\n\n# Generate visualizations\nprint(\"\\nGenerating visualizations...\")\n\n# Text length distributions\nplot_distribution(df_analysis, 'Text_Length', 'Distribution of Text Length by Fraud Status', 'Text Length (characters)')\nplot_boxplot(df_analysis, 'Text_Length', 'Text Length by Fraud Status', 'Text Length (characters)')\n\n# Word count distributions\nplot_distribution(df_analysis, 'Word_Count', 'Distribution of Word Count by Fraud Status', 'Word Count')\nplot_boxplot(df_analysis, 'Word_Count', 'Word Count by Fraud Status', 'Word Count')\n\n# Sentence count distributions\nplot_distribution(df_analysis, 'Sentence_Count', 'Distribution of Sentence Count by Fraud Status', 'Sentence Count')\nplot_boxplot(df_analysis, 'Sentence_Count', 'Sentence Count by Fraud Status', 'Sentence Count')\n\n# Readability metrics\nplot_distribution(df_analysis, 'Flesch_Reading_Ease', 'Distribution of Flesch Reading Ease by Fraud Status', 'Flesch Reading Ease')\nplot_boxplot(df_analysis, 'Flesch_Reading_Ease', 'Flesch Reading Ease by Fraud Status', 'Flesch Reading Ease')\n\n# Sentiment analysis\nplot_distribution(df_analysis, 'Polarity', 'Distribution of Text Polarity by Fraud Status', 'Polarity')\nplot_boxplot(df_analysis, 'Polarity', 'Text Polarity by Fraud Status', 'Polarity')\n\nplot_distribution(df_analysis, 'Subjectivity', 'Distribution of Text Subjectivity by Fraud Status', 'Subjectivity')\nplot_boxplot(df_analysis, 'Subjectivity', 'Text Subjectivity by Fraud Status', 'Subjectivity')\n\n# Correlation heatmap\nprint(\"Generating correlation heatmap...\")\nplt.figure(figsize=(12, 10))\ncorrelation_cols = ['Text_Length', 'Word_Count', 'Sentence_Count', 'Avg_Word_Length',\n                    'Avg_Sentence_Length', 'Flesch_Reading_Ease', 'Flesch_Kincaid_Grade',\n                    'Automated_Readability_Index', 'Polarity', 'Subjectivity', 'Fraud_Binary']\ncorr_matrix = df_analysis[correlation_cols].corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\nsns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n            vmin=-1, vmax=1, square=True, linewidths=.5)\nplt.title('Correlation Heatmap of Text Features', fontsize=15)\nplt.tight_layout()\nplt.savefig('plots/correlation_heatmap.png')\nplt.close()\n\n# Word frequency analysis\nprint(\"Analyzing word frequencies...\")\n\n# Function to get most common words\ndef get_common_words(text_series, n=20, min_word_length=3):\n    \"\"\"Get most common words from a series of texts\"\"\"\n    stop_words = set(stopwords.words('english'))\n    all_words = []\n\n    for text in text_series:\n        words = [word.lower() for word in safe_tokenize(text, word_tokenize)\n                if word.lower() not in stop_words\n                and len(word) >= min_word_length\n                and word.isalpha()]\n        all_words.extend(words)\n\n    return Counter(all_words).most_common(n)\n\n# Get common words for fraud and non-fraud texts\nfraud_common = get_common_words(df_analysis[df_analysis['Fraud'] == 'yes']['Processed_Text'])\nnon_fraud_common = get_common_words(df_analysis[df_analysis['Fraud'] == 'no']['Processed_Text'])\n\n# Plot word frequencies\ndef plot_word_freq(word_counts, title, color, filename):\n    \"\"\"Plot word frequencies\"\"\"\n    plt.figure(figsize=(12, 8))\n    words, counts = zip(*word_counts)\n    sns.barplot(x=list(counts), y=list(words), palette=[color])\n    plt.title(title, fontsize=15)\n    plt.xlabel('Frequency', fontsize=12)\n    plt.ylabel('Words', fontsize=12)\n    plt.tight_layout()\n    plt.savefig(f'plots/{filename}.png')\n    plt.close()\n\nplot_word_freq(fraud_common, 'Most Common Words in Fraudulent Texts', 'red', 'fraud_common_words')\nplot_word_freq(non_fraud_common, 'Most Common Words in Non-Fraudulent Texts', 'blue', 'non_fraud_common_words')\n\n# Generate word clouds\nprint(\"Generating word clouds...\")\n\ndef generate_wordcloud(text_series, title, color_map, filename):\n    \"\"\"Generate and save a word cloud from a series of texts\"\"\"\n    all_text = ' '.join(text_series)\n    wordcloud = WordCloud(width=800, height=400, background_color='white',\n                         max_words=100, colormap=color_map,\n                         contour_width=1, contour_color='steelblue')\n    wordcloud.generate(all_text)\n\n    plt.figure(figsize=(12, 8))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title, fontsize=15)\n    plt.tight_layout()\n    plt.savefig(f'plots/{filename}.png')\n    plt.close()\n\ngenerate_wordcloud(df_analysis[df_analysis['Fraud'] == 'yes']['Processed_Text'],\n                  'Word Cloud - Fraudulent Texts', 'Reds', 'fraud_wordcloud')\ngenerate_wordcloud(df_analysis[df_analysis['Fraud'] == 'no']['Processed_Text'],\n                  'Word Cloud - Non-Fraudulent Texts', 'Blues', 'non_fraud_wordcloud')\n\n# Class Imbalance Handling with SMOTE-NC\nprint(\"\\nDemonstrating class imbalance handling with SMOTE-NC...\")\n# Note: Our dataset is already balanced, but we'll demonstrate SMOTE-NC for educational purposes\n\n# Prepare data for SMOTE-NC\n# We'll use TF-IDF vectorization for the text features\nprint(\"Vectorizing text data...\")\ntfidf = TfidfVectorizer(max_features=1000)\nX_tfidf = tfidf.fit_transform(df_analysis['Processed_Text'])\n\n# Convert to DataFrame for easier handling\nX_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n\n# Add some numerical features\nX_combined = pd.concat([\n    X_tfidf_df,\n    df_analysis[['Text_Length', 'Word_Count', 'Sentence_Count', 'Flesch_Reading_Ease', 'Polarity', 'Subjectivity']]\n], axis=1)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_combined, df_analysis['Fraud_Binary'], test_size=0.3, random_state=42\n)\n\n# Apply SMOTE-NC\n# Identify categorical features (in this case, all TF-IDF features are categorical)\ncategorical_features = list(range(len(tfidf.get_feature_names_out())))\nsmote_nc = SMOTENC(categorical_features=categorical_features, random_state=42)\nX_resampled, y_resampled = smote_nc.fit_resample(X_train, y_train)\n\nprint(f\"Original training data shape: {X_train.shape}, Class distribution: {pd.Series(y_train).value_counts().to_dict()}\")\nprint(f\"Resampled training data shape: {X_resampled.shape}, Class distribution: {pd.Series(y_resampled).value_counts().to_dict()}\")\n\n# Save analysis results to CSV\nprint(\"\\nSaving analysis results...\")\ntry:\n    # Create data directory if it doesn't exist\n    if not os.path.exists('data'):\n        os.makedirs('data')\n    df_analysis.to_csv('data/financial_fraud_analysis.csv', index=False)\n    print(\"Analysis results saved successfully.\")\nexcept Exception as e:\n    print(f\"Error saving analysis results: {e}\")\n\nprint(\"\\nExploratory Data Analysis completed successfully!\")\nprint(\"Results and visualizations have been saved to the 'plots' directory.\")\nprint(\"Detailed analysis data has been saved to 'data/financial_fraud_analysis.csv'.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-05-18T10:13:20.391810Z","iopub.execute_input":"2025-05-18T10:13:20.392146Z","iopub.status.idle":"2025-05-18T10:32:56.203965Z","shell.execute_reply.started":"2025-05-18T10:13:20.392119Z","shell.execute_reply":"2025-05-18T10:32:56.202866Z"}}},{"cell_type":"code","source":"import os, zipfile\nfrom IPython.display import FileLink\n\ndef zipdir(path, ziph):\n    # ziph: a ZipFile handle\n    for root, dirs, files in os.walk(path):\n        for fname in files:\n            filepath = os.path.join(root, fname)\n            # preserve the folder structure under “output/”\n            arcname = os.path.relpath(filepath, os.path.dirname(path))\n            ziph.write(filepath, arcname)\n\noutput_folder = '/kaggle/working/'\nzip_path = 'output.zip'\n\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n    zipdir(output_folder, z)\n\nprint(f\"📦 Packed everything into {zip_path}\")\nFileLink(zip_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T10:44:29.477764Z","iopub.execute_input":"2025-05-18T10:44:29.478160Z","iopub.status.idle":"2025-05-18T10:44:50.477757Z","shell.execute_reply.started":"2025-05-18T10:44:29.478131Z","shell.execute_reply":"2025-05-18T10:44:50.476488Z"}},"outputs":[{"name":"stdout","text":"📦 Packed everything into output.zip\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/output.zip","text/html":"<a href='output.zip' target='_blank'>output.zip</a><br>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}